{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antony/Documents/Visual Studio Code/clip/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/antony/Documents/Visual Studio Code/clip/env/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/antony/Documents/Visual Studio Code/clip/env/lib/python3.10/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizerFast, CLIPImageProcessor, CLIPProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from dataset import TextImagePairSet\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip:CLIPModel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer:CLIPTokenizerFast = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_processor:CLIPImageProcessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor:CLIPProcessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in clip.parameters():\n",
    "    x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(x.numel() for x in clip.visual_projection.parameters()))\n",
    "print(sum(x.numel() for x in clip.text_projection.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TextImagePairSet(\"./data/train/data.csv\", \"./data/train\")\n",
    "devset = TextImagePairSet(\"./data/dev/data.csv\", \"./data/dev\")\n",
    "testset = TextImagePairSet(\"./data/test/data.csv\", \"./data/test\")\n",
    "\n",
    "trainLoader = DataLoader(trainset, batch_size=1)\n",
    "devLoader = DataLoader(devset, batch_size=1)\n",
    "testLoader = DataLoader(testset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "fc = torch.nn.Linear(in_features=768 + 512, out_features=1, bias=True)\n",
    "optimizer = AdamW(fc.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(trainLoader, clip, fc, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.train()\n",
    "    for X, y, label in trainLoader:                       \n",
    "        X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "        l.append(X['pixel_values'])\n",
    "        optimizer.zero_grad()\n",
    "        clip_outputs = clip(**X, **tok_label)\n",
    "        \n",
    "        y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "       \n",
    "        loss = loss_fn(y_hat, torch.Tensor([[y]]))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(devLoader, clip, fc, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, label in devLoader:            \n",
    "            X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "            clip_outputs = clip(**X, **tok_label)\n",
    "            y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testLoader, clip, fc, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, label in testLoader:\n",
    "            X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "            clip_outputs = clip(**X, **tok_label)\n",
    "            y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "            \n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()            \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "train_loss = [0]*10\n",
    "dev_loss = [0]*10\n",
    "cos_sim_fn = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"antes\")\n",
    "    train_loss[i] = train(trainLoader, clip, fc, optimizer, loss_fn)\n",
    "    print(\"depois\")\n",
    "    dev_loss[i] = validate(devLoader, clip, fc, loss_fn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(trainLoader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_image.shape)\n",
    "es_una_label = tokenizer([\"texto maluco que eu nao sei\"], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_una_label.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs = clip(**{\"pixel_values\" : le_image}, **es_una_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs.keys()\n",
    "#y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(clip_outputs['vision_model_output']['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs['logits_per_image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = read_image(\"./data/train/1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = processor(text=\"mulher em um fundo verde fazendo sinais de libras\", images=x, return_tensors=\"pt\", padding=True)\n",
    "x_pross = vision_processor(x, return_tensors=\"pt\")\n",
    "tok_y = tokenizer([\"mulher em um fundo verde fazendo sinais de libras\"], padding=True, return_tensors=\"pt\")\n",
    "outputs = clip(**x_pross, **tok_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits_per_image'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = torch.nn.Linear(in_features=1, out_features=1)\n",
    "z = bl(outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['vision_model_output'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)\n",
    "y_hat = softmax(outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "fc = torch.nn.Linear(in_features=768 + 512, out_features=1, bias=True)\n",
    "optimizer = AdamW(fc.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text = softmax(outputs['logits_per_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor([1, 2, 3 ,4])\n",
    "t2 = torch.Tensor([5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_norm = t/t.norm()\n",
    "print(t_norm)\n",
    "\n",
    "t2_norm = t2/t2.norm()\n",
    "t2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_t2_norm = t_norm * t2_norm\n",
    "norm_t1_t2 = t_norm.norm() * t2_norm.norm()\n",
    "print(t1_t2_norm)\n",
    "print(norm_t1_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = t1_t2_norm / norm_t1_t2\n",
    "print(cos)\n",
    "sum(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "torch_cos = similarity(t, t2)\n",
    "torch_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t * t2)/max(t/t.norm() * t2/t2.norm())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
