{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizerFast, CLIPImageProcessor, CLIPProcessor\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from dataset import TextImagePairSet\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip:CLIPModel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer:CLIPTokenizerFast = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_processor:CLIPImageProcessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor:CLIPProcessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in clip.parameters():\n",
    "    x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(x.numel() for x in clip.visual_projection.parameters()))\n",
    "print(sum(x.numel() for x in clip.text_projection.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TextImagePairSet(\"./data/train/data.csv\", \"./data/train\")\n",
    "devset = TextImagePairSet(\"./data/dev/data.csv\", \"./data/dev\")\n",
    "testset = TextImagePairSet(\"./data/test/data.csv\", \"./data/test\")\n",
    "\n",
    "trainLoader = DataLoader(trainset, batch_size=1)\n",
    "devLoader = DataLoader(devset, batch_size=1)\n",
    "testLoader = DataLoader(testset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "fc = torch.nn.Linear(in_features=768 + 512, out_features=1, bias=True)\n",
    "optimizer = AdamW(fc.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(trainLoader, clip, fc, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.train()\n",
    "    for X, y, label in trainLoader:                       \n",
    "        X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "        l.append(X['pixel_values'])\n",
    "        optimizer.zero_grad()\n",
    "        clip_outputs = clip(**X, **tok_label)\n",
    "        \n",
    "        y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "       \n",
    "        loss = loss_fn(y_hat, torch.Tensor([[y]]))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(devLoader, clip, fc, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, label in devLoader:            \n",
    "            X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "            clip_outputs = clip(**X, **tok_label)\n",
    "            y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testLoader, clip, fc, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    fc.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, label in testLoader:\n",
    "            X, tok_label = vision_processor(X, return_tensors=\"pt\"), tokenizer(label, padding=True, return_tensors=\"pt\")\n",
    "            clip_outputs = clip(**X, **tok_label)\n",
    "            y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])\n",
    "            \n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()            \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "train_loss = [0]*10\n",
    "dev_loss = [0]*10\n",
    "cos_sim_fn = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"antes\")\n",
    "    train_loss[i] = train(trainLoader, clip, fc, optimizer, loss_fn)\n",
    "    print(\"depois\")\n",
    "    dev_loss[i] = validate(devLoader, clip, fc, loss_fn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_image.shape)\n",
    "es_una_label = tokenizer([\"texto maluco que eu nao sei\"], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs = clip(**{\"pixel_values\" : le_image}, **es_una_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs.keys()\n",
    "#y_hat = torch.nn.Softmax(dim=0)(clip_outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(clip_outputs['vision_model_output']['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip_outputs['logits_per_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outputs['logits_per_image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = read_image(\"./data/train/1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=\"mulher em um fundo verde fazendo sinais de libras\", images=x, return_tensors=\"pt\", padding=True)\n",
    "x_pross = vision_processor(x, return_tensors=\"pt\")\n",
    "tok_y = tokenizer([\"mulher em um fundo verde fazendo sinais de libras\"], padding=True, return_tensors=\"pt\")\n",
    "outputs = clip(**x_pross, **tok_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\"model\": \"torch.optim.AdamW\", \"lr\" : 1e-4, 'betas' : (0.9, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = configs.pop(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.optim.AdamW'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adamw.AdamW"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exec(\"import torch\")\n",
    "adam = eval(model)\n",
    "adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('torch.optim.AdamW')(**{}, params=clip.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('torch.optim.AdamW')(clip.parameters()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
