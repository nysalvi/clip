{
  "0" : {
    "hidden_size": "openai/clip-vit-base-patch32",
    "intermediate_size": 3072,
    "projection_dim": 512,
    "num_hidden_layers": 12,
    "num_attention_heads": 12,
    "num_channels": 3,
    "image_size": 224,
    "patch_size": 32,
    "hidden_act": "quick_gelu",
    "layer_norm_eps": 1e-05,
    "attention_dropout": 0.0,
    "initializer_range": 0.02,
    "initializer_factor": 1.0,
    "model_type": "clip_vision_model",
    "transformers_version": "4.41.2"
  }
}