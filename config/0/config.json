{
    "name" : "CLIP",
	"pretrained" : "openai/clip-vit-base-patch32",
    "seed" : false,
    "batch_size" : 16,
    "early_stop" : 5,
    "epoch" : 0,
	"total_epochs" : 10,	    
    "path" : "./data",
    "train" : {
        "eval" : "_datasets.dataset.TextImagePairSet",
        "annotations_file" : "./data/train/data.csv",
        "img_dir" : "./data/train",
        "transform" : "clip_preprocess",
        "target_transform" : false
    },
    "dev" : {
        "eval" : "_datasets.dataset.TextImagePairSet",
        "annotations_file" : "./data/train/data.csv",
        "img_dir" : "./data/train",
        "transform" : "clip_preprocess",
        "target_transform" : false
    },  
    "test" : {
        "eval" : "_datasets.dataset.TextImagePairSet",
        "annotations_file" : "./data/train/data.csv",
        "img_dir" : "./data/train",
        "transform" : "clip_preprocess",
        "target_transform" : false
    },    
	"optimizer" : {
		"eval" : "torch.optim.Adam",
		"lr" : 1e-3,
		"betas" : [0.9, 0.99],
        "weight_decay" : 5e-4
	},
	"loss_fn" : {
		"eval" : "torch.nn.CrossEntropyLoss"
	},
	"lr_scheduler" : {
		"eval" : "torch.optim.lr_scheduler.CosineAnnealingLR",
		"T_max" : 3,
		"verbose" : true
	},
    "metric" : "f1_score",    
	"device" : "cuda:0"		
}
